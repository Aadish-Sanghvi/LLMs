{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT2 tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Token IDs: tensor([[ 464, 2003,  286, 9552,  318]])\n",
      "üîπ Tokenized Words: ['The', ' future', ' of', ' AI', ' is']\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 1: Tokenize a simple input sentence\n",
    "sentence = \"The future of AI is\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "print(\"üîπ Token IDs:\", inputs[\"input_ids\"])\n",
    "print(\"üîπ Tokenized Words:\", [tokenizer.decode([token]) for token in inputs[\"input_ids\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Decoded Sentence: The future of AI is\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 2: Decode the input IDs back to text\n",
    "decoded = tokenizer.decode(inputs[\"input_ids\"][0])\n",
    "print(\"\\nüîÅ Decoded Sentence:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 3: Generate text from the input prompt\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=1.2,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Generated Text:\n",
      " The future of AI is going to be heavily linked to whether we can develop new ways to measure intelligence in artificial systems.\n",
      "\n",
      "\"It's one thing to make sure the systems are intelligent,\" says Professor Steve Kawa, Professor of Artificial Intelligence in\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 4: Decode generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\nü§ñ Generated Text:\\n\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
